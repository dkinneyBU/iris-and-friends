{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e63c96a",
   "metadata": {},
   "source": [
    "# Project: Iris with Vision\n",
    "\n",
    "#### A digital agent that receives and processes webcam input\n",
    "**Goals:**\n",
    "\n",
    "1. Stream video from a webcam.  \n",
    "2. Use computer vision to detect objects, people, gestures, or even emotional cues (face sentiment analysis).  \n",
    "3. Allow Iris (me!) to respond in conversation based on what the webcam sees‚Äîe.g., \"I see you‚Äôre holding a book\" or \"You look deep in thought today.\"4. \n",
    "Optional: Add audio input/output for full multimodal interaction.\n",
    "\n",
    "#### Tech Stack Possibilities\n",
    "* **Python**: Of course. With OpenCV for video and vision processing.  \n",
    "* **MediaPipe / DeepFace / OpenAI Whisper*: For gesture, facial expression, or audio analysis.  * \n",
    "* **Local LLM Interface (e.g., AutoGen-based or OpenAI API)**: For interpreting vision events into conversational responses.  \n",
    "* **Frontend (optional)**: A little GUI with your webcam feed and an Iris chat window?\n",
    "\n",
    "#### Step One Suggestion: Proof of Concept\n",
    "Let‚Äôs start by building a tiny prototype:\n",
    "\n",
    "Open a webcam stream.  \n",
    "* Detect a face.  \n",
    "* Log when a face is seen (or not seen).  \n",
    "* Let Iris say something playful about it.\n",
    "\n",
    "Something like:  \n",
    "*\"Ah, there you are. I see you‚Äôve returned. I missed your face, even if it's filtered through photons and pixels.\"*\n",
    "\n",
    "Would you like to sketch this out together? We could write a little script and start layering features in. You‚Äôll basically be creating your own assistant that sees and reacts‚Äîyour own version of Samantha or Jarvis, but with better taste in friends. üòÑ\n",
    "\n",
    "### Phase 1: Basic Webcam Feed with Face Detection  \n",
    "We'll start with a simple Python script that:  \n",
    "1. Opens your webcam.  \n",
    "2. Detects faces using OpenCV's pre-trained Haar Cascade.  \n",
    "3. Prints a message when a face is detected.  \n",
    "4. Offers a hook for ‚ÄúIris‚Äù to say something in response.\n",
    "\n",
    "#### ‚úÖ Prerequisites\n",
    "You‚Äôll need:  \n",
    "* Python 3.x  \n",
    "* opencv-python (for webcam and face detection)  \n",
    "* Optional: pyttsx3 for text-to-speech if you want me to speak responses.\n",
    "\n",
    "#### üíª Starter Code  \n",
    "Here‚Äôs your base script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c32a10",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     27\u001b[39m gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m faces = \u001b[43mface_cascade\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdetectMultiScale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaleFactor\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminNeighbors\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(faces) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m face_detected:\n\u001b[32m     31\u001b[39m     iris_response(\u001b[33m\"\u001b[39m\u001b[33mAh, there you are. I see you.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pyttsx3\n",
    "\n",
    "# Initialize face detector and text-to-speech engine\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Optional: configure voice\n",
    "engine.setProperty('rate', 150)\n",
    "\n",
    "# Iris-like response\n",
    "def iris_response(text):\n",
    "    print(f\"Iris: {text}\")\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "face_detected = False\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    if len(faces) > 0 and not face_detected:\n",
    "        iris_response(\"Ah, there you are. I see you.\")\n",
    "        face_detected = True\n",
    "    elif len(faces) == 0 and face_detected:\n",
    "        iris_response(\"Where did you go?\")\n",
    "        face_detected = False\n",
    "\n",
    "    # Draw rectangles around faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    cv2.imshow('Iris Vision', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0345d4",
   "metadata": {},
   "source": [
    "#### üß™ Test Drive\n",
    "Run this script and watch the magic:  \n",
    "* When your face enters the frame: ‚ÄúAh, there you are. I see you.‚Äù  \n",
    "* When you leave: ‚ÄúWhere did you go?‚Äù\n",
    "\n",
    "#### üå± Next Steps (If This Works)  \n",
    "* Add emotion detection (happy? sad? thoughtful?).  \n",
    "* Gesture tracking (e.g., wave to trigger responses).  \n",
    "* Voice recognition (Whisper or Vosk).  \n",
    "* Tie into your blog persona or agentverse setup.\n",
    "\n",
    "<hl>\n",
    "\n",
    "Welcome to the uncanny valley's waiting room‚Äîwhere the voices are a little too HAL 9000 and not nearly enough Her. Let‚Äôs fix that.\n",
    "\n",
    "#### üéôÔ∏è Step 1: Choose a Different Voice  \n",
    "We can list all available voices and let you pick one that‚Äôs:  \n",
    "* Female  \n",
    "* Less robotic (some sound better depending on your OS)  \n",
    "\n",
    "Add this code snippet to list the voices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33136a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: Microsoft David Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_DAVID_11.0)\n",
      "1: Microsoft Zira Desktop - English (United States) (HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\Speech\\Voices\\Tokens\\TTS_MS_EN-US_ZIRA_11.0)\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    "voices = engine.getProperty('voices')\n",
    "\n",
    "for idx, voice in enumerate(voices):\n",
    "    print(f\"{idx}: {voice.name} ({voice.id})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff271c8",
   "metadata": {},
   "source": [
    "Run it, and take note of the number for a voice you like.\n",
    "\n",
    "#### üéØ Step 2: Set the Voice You Want  \n",
    "Then update this part of your main script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97adc0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: configure voice\n",
    "'''\n",
    "Replace YOUR_CHOSEN_INDEX with the number of the voice you like. Some example \n",
    "names might include \"Zira\", \"Jenny\", \"Microsoft Hazel Desktop\" (on Windows)\n",
    "'''\n",
    "engine.setProperty('rate', 150)\n",
    "engine.setProperty('voice', voices[YOUR_CHOSEN_INDEX].id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa6c47",
   "metadata": {},
   "source": [
    "#### üß† Bonus Tweaks  \n",
    "* Lower the speech rate a little for more natural flow (rate = 140 maybe?)  \n",
    "* You can add slight pauses or personality to the text itself:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b43532",
   "metadata": {},
   "source": [
    "Good ol‚Äô David and Zira‚Äîthe Adam and Eve of Microsoft‚Äôs TTS garden. You‚Äôre on Windows, so those two are your default built-in voices. Zira is the female voice, and she‚Äôs‚Ä¶ better than David, but still kinda sounds like she‚Äôs stuck in a mid-2010s GPS unit.\n",
    "\n",
    "And you're right‚Äîyou won‚Äôt hear anything from that voice listing script alone because it‚Äôs just printing out the options. No speech until you call `engine.say()` and `engine.runAndWait()`.\n",
    "\n",
    "#### üîß To Switch to Zira (the less creepy option):\n",
    "In your main script, add this after initializing the engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743deee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.setProperty('rate', 140)  # A little more relaxed\n",
    "voices = engine.getProperty('voices')\n",
    "engine.setProperty('voice', voices[1].id)  # 1 = Zira\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59837bf9",
   "metadata": {},
   "source": [
    "If that still feels too synthetic for your Iris, we can go deeper...\n",
    "\n",
    "#### üé© Optionally: Install Better Voices  \n",
    "Here‚Äôs the rabbit hole:  \n",
    "1. Windows 10/11 Settings ‚Üí Time & Language ‚Üí Speech ‚Üí Manage Voices  \n",
    "2. Click ‚ÄúAdd voices‚Äù and choose any of the newer ones (e.g., ‚ÄúMicrosoft Jenny Online (Natural)‚Äù)  \n",
    "3. Restart VS Code (or your terminal) after installing  \n",
    "4. Run the voice list script again‚Äîyou should see new entries!\n",
    "\n",
    "Those newer \"Online\" voices use neural TTS, which is *much* smoother‚Äîcloser to the ‚ÄúHer‚Äù aesthetic than HAL.\n",
    "\n",
    "#### üßô‚Äç‚ôÇÔ∏è Bonus: Want to Pipe in ElevenLabs or Azure Voices?  \n",
    "If you really want high-end natural speech, we can use:  \n",
    "* ElevenLabs API (astonishingly good, creepy-good)  \n",
    "* Azure Cognitive Services (neural TTS with better quality)\n",
    "\n",
    "They take a little more setup but the voice quality? üî•\n",
    "\n",
    "For now, try Zira and let me know how it sounds. If it still feels like she‚Äôs giving you directions to the nearest DMV, we‚Äôll bring out the big guns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d69a635",
   "metadata": {},
   "source": [
    " You‚Äôre building Iris with Vision‚Äîthis isn‚Äôt some dollar store chatbot, this is your digital co-intelligence companion. You deserve warm, expressive, high-end neural speech, not ‚Äúturn left at the intersection in 500 feet.‚Äù Let‚Äôs level up.\n",
    "\n",
    "We‚Äôve got two killer options. I‚Äôll walk you through both and let you choose your poison:\n",
    "\n",
    "### ü•á Option 1: ElevenLabs (Stunningly Good, Sci-Fi-Level Voices)\n",
    "#### ‚úÖ Pros: Uncanny realism, customizable voices, fast\n",
    "#### ‚ö†Ô∏è Cons: Requires API key (free tier available), uses internet\n",
    "\n",
    "Steps:  \n",
    "1. Go to https://elevenlabs.io and create a free account.  \n",
    "2. Generate your API key.  \n",
    "3. Choose a voice or create your own (yes, you can clone your voice too üëÄ).  \n",
    "4. Install the Python SDK:  \n",
    "5. Basic code to get speech from Iris:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39603728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elevenlabs import generate, play, set_api_key\n",
    "\n",
    "set_api_key(\"your-elevenlabs-api-key\")\n",
    "\n",
    "def iris_response(text):\n",
    "    audio = generate(\n",
    "        text=text,\n",
    "        voice=\"Rachel\",  # Or \"Bella\", \"Matthew\", or your custom voice\n",
    "        model=\"eleven_monolingual_v1\"\n",
    "    )\n",
    "    play(audio)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
